# ==== TH∆Ø VI·ªÜN ====
import tensorflow as tf
from tensorflow.keras import layers, models                     # D√πng ƒë·ªÉ x√¢y m√¥ h√¨nh Keras
from tensorflow.keras.applications import EfficientNetB0       # Import EfficientNetB0 ƒë√£ hu·∫•n luy·ªán s·∫µn
import matplotlib.pyplot as plt                                 # V·∫Ω bi·ªÉu ƒë·ªì
import pandas as pd                                             # Ghi log k·∫øt qu·∫£ d·∫°ng b·∫£ng
import os                                                       # L√†m vi·ªác v·ªõi file/th∆∞ m·ª•c
from datetime import datetime                                   # ƒê·ªÉ ƒë·∫∑t timestamp file

# ==== C·∫§U H√åNH ====
IMG_SIZE = (128, 128)                                           # K√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu v√†o
BATCH_SIZE = 32                                                 # S·ªë l∆∞·ª£ng ·∫£nh m·ªói batch khi hu·∫•n luy·ªán
EPOCHS = 30                                                     # S·ªë v√≤ng l·∫∑p train ban ƒë·∫ßu
DATASET_DIR = r"D:\DuLieuHo\dataset"                            # ƒê∆∞·ªùng d·∫´n ƒë·∫øn dataset ƒë√£ chia s·∫µn train/val/test
MODEL_NAME = "efficientnet_cough_model"                         # T√™n m√¥ h√¨nh
OUTPUT_FOLDER = r"G:\My Drive\T√†i li·ªáu NCKH\AIData"             # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£, log, model

# ==== T·∫¢I D·ªÆ LI·ªÜU ====
# Load d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c train
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(DATASET_DIR, "train"),
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

# Load d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c val
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(DATASET_DIR, "val"),
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

# Load d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c test
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(DATASET_DIR, "test"),
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

# L·∫•y danh s√°ch t√™n l·ªõp (class) v√† s·ªë l∆∞·ª£ng l·ªõp
class_names = train_ds.class_names
num_classes = len(class_names)
print("üìÇ C√°c l·ªõp:", class_names)

# ==== T·ªêI ∆ØU D·ªÆ LI·ªÜU ====
AUTOTUNE = tf.data.AUTOTUNE
# S·ª≠ d·ª•ng ti·ªÅn x·ª≠ l√Ω song song ƒë·ªÉ tƒÉng t·ªëc train
train_ds = train_ds.prefetch(AUTOTUNE)
val_ds = val_ds.prefetch(AUTOTUNE)
test_ds = test_ds.prefetch(AUTOTUNE)

# ==== KH·ªûI T·∫†O M√î H√åNH EfficientNetB0 ====
# T·∫£i EfficientNetB0 pretrained t·ª´ ImageNet, kh√¥ng bao g·ªìm layer ƒë·∫ßu ra cu·ªëi (include_top=False)
base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(128, 128, 3))
base_model.trainable = False   # Freeze to√†n b·ªô m√¥ h√¨nh ban ƒë·∫ßu (ch∆∞a fine-tune)

# X√¢y d·ª±ng m√¥ h√¨nh m·ªõi tr√™n n·ªÅn EfficientNet
model = models.Sequential([
    layers.Rescaling(1./255),                      # Chu·∫©n h√≥a ·∫£nh t·ª´ [0‚Äì255] v·ªÅ [0‚Äì1]
    base_model,                                    # Th√™m EfficientNet l√†m feature extractor
    layers.GlobalAveragePooling2D(),               # Gi·∫£m chi·ªÅu feature map
    layers.Dropout(0.2),                           # Tr√°nh overfitting b·∫±ng dropout
    layers.Dense(num_classes, activation='softmax')  # Layer ph√¢n lo·∫°i ƒë·∫ßu ra
])

# Bi√™n d·ªãch m√¥ h√¨nh l·∫ßn ƒë·∫ßu
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',   # Do label l√† d·∫°ng s·ªë
              metrics=['accuracy'])                     # ƒê√°nh gi√° theo ƒë·ªô ch√≠nh x√°c

# ==== HU·∫§N LUY·ªÜN L·∫¶N 1 ====
history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)  # Train m√¥ h√¨nh

# ==== FINE-TUNE ====
base_model.trainable = True  # M·ªü kh√≥a m√¥ h√¨nh EfficientNet
# Ch·ªâ fine-tune t·∫ßng cu·ªëi (20 layer cu·ªëi c√πng), c√≤n l·∫°i gi·ªØ frozen
for layer in base_model.layers[:-20]:
    layer.trainable = False

# Bi√™n d·ªãch l·∫°i m√¥ h√¨nh sau khi fine-tune
model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),   # Learning rate th·∫•p h∆°n
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train l·∫°i m√¥ h√¨nh th√™m 10 epoch (fine-tune)
history_fine = model.fit(train_ds, validation_data=val_ds, epochs=10)

# ==== GHI LOG K·∫æT QU·∫¢ ====
def save_training_log(history, history_fine=None, output_folder=OUTPUT_FOLDER, prefix=MODEL_NAME):
    # G·ªôp l·ªãch s·ª≠ train & fine-tune
    history_data = history.history
    if history_fine:
        for key in history_fine.history:
            history_data[key].extend(history_fine.history[key])

    # T·∫°o DataFrame ch·ª©a log
    df = pd.DataFrame({
        'Epoch': list(range(1, len(history_data['accuracy']) + 1)),
        'Accuracy': history_data['accuracy'],
        'Loss': history_data['loss'],
        'Val Accuracy': history_data['val_accuracy'],
        'Val Loss': history_data['val_loss']
    })

    os.makedirs(output_folder, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  # T·∫°o timestamp hi·ªán t·∫°i
    # T·∫°o t√™n file log
    excel_path = os.path.join(output_folder, f"{prefix}_trainlog_{timestamp}.xlsx")
    acc_plot = os.path.join(output_folder, f"{prefix}_accuracy_{timestamp}.png")
    loss_plot = os.path.join(output_folder, f"{prefix}_loss_{timestamp}.png")

    df.to_excel(excel_path, index=False)  # Ghi file Excel
    print("‚úÖ ƒê√£ l∆∞u file Excel log:", excel_path)

    # V·∫Ω bi·ªÉu ƒë·ªì Accuracy
    plt.figure(figsize=(8,5))
    plt.plot(df['Epoch'], df['Accuracy'], label='Train Acc')
    plt.plot(df['Epoch'], df['Val Accuracy'], label='Val Acc')
    plt.title('Accuracy theo Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()
    plt.tight_layout()
    plt.savefig(acc_plot)   # L∆∞u ·∫£nh
    plt.close()
    print("‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì Accuracy:", acc_plot)

    # V·∫Ω bi·ªÉu ƒë·ªì Loss
    plt.figure(figsize=(8,5))
    plt.plot(df['Epoch'], df['Loss'], label='Train Loss')
    plt.plot(df['Epoch'], df['Val Loss'], label='Val Loss')
    plt.title('Loss theo Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid()
    plt.tight_layout()
    plt.savefig(loss_plot)
    plt.close()
    print("‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì Loss:", loss_plot)

# G·ªçi h√†m l∆∞u log v√† bi·ªÉu ƒë·ªì
save_training_log(history, history_fine)

# ==== L∆ØU M√î H√åNH ====
MODEL_FILE = os.path.join(OUTPUT_FOLDER, f"{MODEL_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.keras")
model.save(MODEL_FILE)
print("‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh:", MODEL_FILE)
