# ===========================================
# Cài đặt thư viện cần thiết
# ===========================================
!pip install -q xgboost numpy pandas matplotlib seaborn openpyxl pytz scikit-learn tensorflow tensorflow-hub librosa soundfile

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import pytz
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from xgboost import XGBClassifier
import tensorflow_hub as hub
import soundfile as sf
import librosa

# ===========================================
# Tạo thư mục output để lưu kết quả
# ===========================================
tz = pytz.timezone('Asia/Ho_Chi_Minh')
timestamp = datetime.now(tz).strftime("%Y%m%d_%H%M%S")
OUTPUT_DIR = f'/content/drive/MyDrive/Tai_Lieu_NCKH/AIData/xgboost/{timestamp}'
os.makedirs(OUTPUT_DIR, exist_ok=True)
print("Thư mục output:", OUTPUT_DIR)

# ===========================================
# Trích xuất feature từ file wav
# (YAMNet + MFCC + delta + delta-delta + tabular)
# ===========================================
yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')

input_wav_dir = '/content/drive/MyDrive/Tai_Lieu_NCKH/Dataset/wav_dataset'
output_npy_dir = os.path.join(OUTPUT_DIR, "combined_features")
os.makedirs(output_npy_dir, exist_ok=True)

def extract_combined_feature(wav_path):
    wav_data, sr = sf.read(wav_path)
    if len(wav_data.shape) > 1:
        wav_data = np.mean(wav_data, axis=1)
    if sr != 16000:
        wav_data = librosa.resample(wav_data, orig_sr=sr, target_sr=16000)

    # YAMNet
    scores, embeddings, spectrogram = yamnet_model(wav_data)
    mean_yamnet = np.mean(embeddings.numpy(), axis=0)

    # MFCC
    mfcc = librosa.feature.mfcc(y=wav_data, sr=16000, n_mfcc=13)
    mfcc_mean = np.mean(mfcc, axis=1)
    delta = librosa.feature.delta(mfcc)
    delta2 = librosa.feature.delta(mfcc, order=2)
    delta_mean = np.mean(delta, axis=1)
    delta2_mean = np.mean(delta2, axis=1)

    # extra features
    rolloff_mean = np.mean(librosa.feature.spectral_rolloff(y=wav_data, sr=16000))
    zcr_mean = np.mean(librosa.feature.zero_crossing_rate(y=wav_data))
    rms_mean = np.mean(librosa.feature.rms(y=wav_data))
    duration = librosa.get_duration(y=wav_data, sr=16000)
    centroid = np.mean(librosa.feature.spectral_centroid(y=wav_data, sr=16000))
    mfcc_mean_1 = np.mean(mfcc[0])
    mfcc_mean_2 = np.mean(mfcc[1])
    chroma = np.mean(librosa.feature.chroma_stft(y=wav_data, sr=16000))

    combined_feature = np.concatenate([
        mean_yamnet, mfcc_mean, delta_mean, delta2_mean,
        [rolloff_mean, zcr_mean, rms_mean, duration, centroid, mfcc_mean_1, mfcc_mean_2, chroma]
    ])
    return combined_feature

# Extract
for label_name in sorted(os.listdir(input_wav_dir)):
    label_folder = os.path.join(input_wav_dir, label_name)
    if not os.path.isdir(label_folder):
        continue
    output_label_folder = os.path.join(output_npy_dir, label_name)
    os.makedirs(output_label_folder, exist_ok=True)
    for file in os.listdir(label_folder):
        if file.lower().endswith('.wav'):
            feature = extract_combined_feature(os.path.join(label_folder, file))
            np.save(os.path.join(output_label_folder, file.replace('.wav', '.npy')), feature)
    print("Xong lớp:", label_name)
print("Hoàn thành trích xuất feature.")

# ===========================================
# Tải dataset vào X, y
# ===========================================
label_names = sorted([d for d in os.listdir(output_npy_dir) if os.path.isdir(os.path.join(output_npy_dir, d))])
label_map = {name: idx for idx, name in enumerate(label_names)}
print("Label mapping:", label_map)

X, y = [], []
for label_name in label_names:
    folder = os.path.join(output_npy_dir, label_name)
    for file in os.listdir(folder):
        if file.endswith(".npy"):
            X.append(np.load(os.path.join(folder, file)).flatten())
            y.append(label_map[label_name])
X, y = np.array(X), np.array(y)
print(f"Kích thước dataset: {X.shape}, Nhãn: {y.shape}")

# ===========================================
# Tiền xử lý - StandardScaler
# ===========================================
scaler = StandardScaler()
X = scaler.fit_transform(X)

# ===========================================
# Chia train/val/test
# ===========================================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)

# ===========================================
# Huấn luyện GridSearchCV cho XGBoost
# ===========================================
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.85],
    'colsample_bytree': [0.7, 0.85],
    'gamma': [0, 0.3],
}

xgb_clf = XGBClassifier(
    objective='multi:softprob',
    num_class=5,
    n_estimators=200,
    eval_metric='mlogloss',
    use_label_encoder=False
)

grid_search = GridSearchCV(
    estimator=xgb_clf,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3,
    verbose=2,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("Best params:", grid_search.best_params_)
print("Best CV accuracy:", grid_search.best_score_)

# ===========================================
# Đánh giá trên tập validation và test
# ===========================================
best_model = grid_search.best_estimator_

y_val_pred = best_model.predict(X_val)
y_test_pred = best_model.predict(X_test)

print("Validation Report:")
print(classification_report(y_val, y_val_pred, target_names=label_names))
print("Test Report:")
print(classification_report(y_test, y_test_pred, target_names=label_names))

# ===========================================
# Vẽ confusion matrix
# ===========================================
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8,8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)
plt.title("Ma trận nhầm lẫn (XGBoost + GridSearchCV)")
plt.savefig(os.path.join(OUTPUT_DIR, f"{timestamp}_confusion_matrix.png"))
plt.show()

# ===========================================
# Lưu model và scaler
# ===========================================
import joblib
joblib.dump(best_model, os.path.join(OUTPUT_DIR, f"{timestamp}_xgboost_model.pkl"))
joblib.dump(scaler, os.path.join(OUTPUT_DIR, f"{timestamp}_scaler.pkl"))

print("Đã lưu toàn bộ model, scaler và kết quả tại:", OUTPUT_DIR)
