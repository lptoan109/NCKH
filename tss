# ====== CÀI ĐẶT THƯ VIỆN ======
!pip install tensorflow pandas matplotlib openpyxl tqdm pytz scikit-learn

# ====== KẾT NỐI GOOGLE DRIVE ======
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# ====== THƯ VIỆN ======
import os, random, shutil
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import EfficientNetV2S
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from datetime import datetime
import pytz
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# ====== CẤU HÌNH ======
tz = pytz.timezone('Asia/Ho_Chi_Minh')
timestamp = datetime.now(tz).strftime("%Y%m%d_%H%M%S")
OUTPUT_SPLIT_DIR = '/content/drive/MyDrive/Tai_Lieu_NCKH/Dataset/coughvid_datasetfull'
RESULT_PARENT_DIR = '/content/drive/MyDrive/Tai_Lieu_NCKH/AIData/efficientnetv2s'
OUTPUT_DIR = os.path.join(RESULT_PARENT_DIR, timestamp)
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f" Thư mục lưu kết quả lần train này: {OUTPUT_DIR}")

IMG_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 50
MODEL_NAME = "efficientnetv2s_cough_model"

# ====== LOAD DATA ======
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(OUTPUT_SPLIT_DIR, "train"), image_size=IMG_SIZE, batch_size=BATCH_SIZE
)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(OUTPUT_SPLIT_DIR, "val"), image_size=IMG_SIZE, batch_size=BATCH_SIZE
)
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(OUTPUT_SPLIT_DIR, "test"), image_size=IMG_SIZE, batch_size=BATCH_SIZE
)
class_names = train_ds.class_names
num_classes = len(class_names)
print("Các lớp:", class_names)

# ====== THÊM DATA AUGMENTATION ======
def add_noise_to_image(image, label, noise_factor=0.005):
    image = tf.cast(image, tf.float32) / 255.0
    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=1.0)
    noisy_image = image + noise_factor * noise
    noisy_image = tf.clip_by_value(noisy_image, 0.0, 1.0)
    return noisy_image, label

def combined_augment(image, label):
    image, label = add_noise_to_image(image, label)
    return image, label

train_ds = train_ds.map(combined_augment, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(tf.data.AUTOTUNE)
test_ds = test_ds.prefetch(tf.data.AUTOTUNE)

# ====== CLASS WEIGHT ======
labels = [int(y.numpy()) for _, y in train_ds.unbatch()]
class_weights = compute_class_weight("balanced", classes=np.arange(num_classes), y=labels)
class_weight_dict = {i: w for i, w in enumerate(class_weights)}
print("Class weights:", class_weight_dict)

# ====== MÔ HÌNH EfficientNetV2-S ======
base_model = EfficientNetV2S(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False

model = models.Sequential([
    layers.Rescaling(1./255),
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# ====== CALLBACK (cho freeze và fine-tune 50 lớp) ======
callbacks = [
    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
]

# ====== TRAIN LẦN 1 (Freeze base) ======
history = model.fit(
    train_ds, validation_data=val_ds, epochs=EPOCHS,
    callbacks=callbacks, class_weight=class_weight_dict
)

# ====== FINE-TUNE 50 LỚP CUỐI ======
base_model.trainable = True
for layer in base_model.layers[:-50]:
    layer.trainable = False

model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history_fine = model.fit(
    train_ds, validation_data=val_ds, epochs=30,
    callbacks=callbacks, class_weight=class_weight_dict
)

# ====== FINE-TUNE TOÀN BỘ BASE_MODEL VỚI CosineDecay ======
print("Bắt đầu fine-tune toàn bộ EfficientNetV2-S (dùng CosineDecay)...")
base_model.trainable = True

cosine_scheduler = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate=1e-4,
    decay_steps=50,
    alpha=1e-2
)

optimizer = tf.keras.optimizers.Adam(learning_rate=cosine_scheduler)

fine_callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
]

model.compile(
    optimizer=optimizer,
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history_full_fine = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=50,
    callbacks=fine_callbacks,
    class_weight=class_weight_dict
)

# ====== ĐÁNH GIÁ TRÊN TEST SET ======
test_loss, test_acc = model.evaluate(test_ds)
print(f" Test Accuracy sau fine-tune toàn bộ: {test_acc*100:.2f}%")

# ====== CONFUSION MATRIX ======
y_true = np.concatenate([y.numpy() for _, y in test_ds], axis=0)
y_pred = np.argmax(model.predict(test_ds), axis=1)
print("\nClassification Report (Fine-tune Full):\n", classification_report(y_true, y_pred, target_names=class_names))

# ====== LƯU LOG + VÀ BIỂU ĐỒ ======
def save_training_log(history, history_fine=None, history_full_fine=None, output_folder=OUTPUT_DIR, prefix=MODEL_NAME):
    history_data = history.history
    if history_fine:
        for key in history_fine.history:
            history_data[key].extend(history_fine.history[key])
    if history_full_fine:
        for key in history_full_fine.history:
            history_data[key].extend(history_full_fine.history[key])

    df = pd.DataFrame({
        'Epoch': list(range(1, len(history_data['accuracy']) + 1)),
        'Accuracy': history_data['accuracy'],
        'Loss': history_data['loss'],
        'Val Accuracy': history_data['val_accuracy'],
        'Val Loss': history_data['val_loss']
    })

    excel_path = os.path.join(output_folder, f"{prefix}_trainlog.xlsx")
    acc_plot = os.path.join(output_folder, f"{prefix}_accuracy.png")
    loss_plot = os.path.join(output_folder, f"{prefix}_loss.png")

    df.to_excel(excel_path, index=False)

    # Lưu thêm sheet kết quả test
    test_results_df = pd.DataFrame({
        'True Label': [class_names[i] for i in y_true],
        'Predicted Label': [class_names[i] for i in y_pred]
    })
    with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a') as writer:
        test_results_df.to_excel(writer, index=False, sheet_name='Test Results')

    plt.figure(figsize=(8, 5))
    plt.plot(df['Epoch'], df['Accuracy'], label='Train Accuracy')
    plt.plot(df['Epoch'], df['Val Accuracy'], label='Val Accuracy')
    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.grid()
    plt.savefig(acc_plot); plt.close()

    plt.figure(figsize=(8, 5))
    plt.plot(df['Epoch'], df['Loss'], label='Train Loss')
    plt.plot(df['Epoch'], df['Val Loss'], label='Val Loss')
    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid()
    plt.savefig(loss_plot); plt.close()

    print(f" Đã lưu log, biểu đồ và kết quả test vào: {output_folder}")

save_training_log(history, history_fine, history_full_fine)

# ====== LƯU MÔ HÌNH CUỐI CÙNG ======
MODEL_FILE = os.path.join(OUTPUT_DIR, f"{MODEL_NAME}_finetune_full.keras")
model.save(MODEL_FILE)
print(" Đã lưu mô hình sau fine-tune toàn bộ:", MODEL_FILE)

# ====== LƯU CONFUSION MATRIX VÀO FILE ======
cm = confusion_matrix(y_true, y_pred)
cm_plot_path = os.path.join(OUTPUT_DIR, f"{MODEL_NAME}_confusion_matrix.png")
fig, ax = plt.subplots(figsize=(8, 8))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap='Blues', ax=ax)
plt.title("Confusion Matrix (Fine-tune Full)")
plt.savefig(cm_plot_path)
plt.close()
print(f" Đã lưu confusion matrix vào: {cm_plot_path}")
